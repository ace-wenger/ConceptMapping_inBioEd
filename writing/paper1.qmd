---
title: "Concept Mapping in Biology Education: A Systematic Review and Meta-Analysis"
shorttitle: "Concept Mapping in Biology Education"
date: 09-21-23
author: "Aaron Wenger"
abstract: "This is where the abstract will go"
format: 
  wordcount-pdf:
    keep-tex: true
    include-in-header: 
      - text: \usepackage{enumerate}
  tandf-pdf:
    keep-tex: true
  apaquarto-docx: default
execute:
  echo: false
  warning: false
bibliography: disspaper1.bib
csl: apa.csl
---

```{r}
#| include: false

library(tidyverse)
library(gt)

my_apa_style <- function(x) {
  tab_options(x,
      row_group.as_column = TRUE
  #   # Getting the borders right...
  #   table.border.top.style = "hidden",
  #   column_labels.border.top.width = 2.1,
  #   column_labels.border.top.color = "black",
  #   column_labels.border.bottom.width = 2.1,
  #   column_labels.border.bottom.color = "black",
  #   table_body.border.bottom.width = 2,
  #   table_body.border.bottom.color = "black",
  #   table_body.hlines.color = "white",
  #   row_group.border.top.color = "white",
  #   row_group.border.bottom.color = "white",
  #   stub.border.color = "white",
  #   # # hide the bottom-most line or footnotes will have a border
  #   # !table.border.bottom.color = "white",
  #   # !table.border.bottom.width = px(3),
  #   # table.border.bottom.style = "hidden",
  #           
  #   # make the title size match the body text
  #   heading.title.font.size = px(16),
  #   heading.subtitle.font.size = px(16),
  #   # # make the width 100%
  #   # table.width = pct(100),
  #   # other options
  #   row_group.default_label = "",
  #   heading.align = "left"
  ) %>%
  tab_style(
    style = cell_borders(
      sides = c("top", "bottom", "left", "right"),
      style = NULL
    ),
    locations = list(
      cells_body(),
      cells_stub(), 
      cells_row_groups()
    )
  ) |> 
  tab_style(
    style = cell_borders(
      sides = "bottom",
      style = "solid"
    ),
    locations = cells_column_labels()
  ) |> 
  # removing NA values
  sub_missing(
    columns = everything(), rows = everything(), 
    missing_text = ""
  ) %>%
  # All cells: applying font
  tab_style(
    style = cell_text(font = "Times New Roman"),
    locations = list(
      cells_body(everything(), everything()),
      cells_row_groups(groups = everything()),
      cells_column_labels(columns = everything()),
      cells_stub(rows = everything()),
      cells_stubhead(),
      cells_title(groups = c("title", "subtitle")),
      cells_column_spanners(),
      cells_source_notes()
    )
  ) %>%
  # Row group cells
  tab_style(
    style = cell_text(align = "left"),
    locations = cells_row_groups(groups = everything())
  ) %>%
  # # Table header: applying bold font
  # tab_style(
  #   style = cell_text(weight = "bold"), 
  #   locations = list(
  #     cells_column_labels(columns = everything()),
  #     cells_stubhead()
  #   )
  # ) %>%
  cols_align(align = "center") %>%
  cols_align(align = "left", columns = stub()) %>%
  opt_vertical_padding(scale = 0.25)
}
```


<!--
::: {.content-hidden when-format="pdf"}

{{< include _extensions/wjschne/apaquarto/_apa_title.qmd >}}

:::
-->

Concept mapping (CM) is an instructional activity invented in the 1970s by Novak and his Cornell University research team [@NCtoc2007]. 
A concept map consists of conceptual nodes with connecting verbal links (see Figure 1). 
Each node-link-node connection forms a proposition. 
CM has been applied in a variety of ways and for a variety of instructional purposes such as collaborative learning, group discussion, directed reading, and formative assessment. 
With the move of classes to online settings during the COVID-19 pandemic, CM has received even more attention in science education as a tool for active learning in virtual settings [@GCConstructingOnlineConcept2021; @ChoSynchronousCollaborativeOnline2020]. 

![An example concept map with its key features, from @NCtoc2007](example_concept_map.png)

<!--
```{r}
#| apa-cap: This is an imported graphic.
#| apa-note: My note.
knitr::include_graphics(here::here("writing", "example_concept_map.png"))
```
-->

Over the last 40 years, classroom intervention studies with CM have proliferated and their results vary widely, often conflicting. 
Based on the results of recent meta-analyses and reviews, it is apparent that there is unexplained variance in the observed effects of CM in instructional contexts.
It is an open question whether some of this variance is due to the influence of factors which affect the efficacy of CM, of which we know little [@SNAAStudyingConstructingConcept2018a; @KinConceptMappingLearning2014; @PYVCMappingLearningStrategy2012a], or other study-specific factors (e.g., study design, comparison condition).

The present work is a systematic review and meta-analysis with the primary purpose of learning more about the factors that influence the effect size of CM interventions in empirical studies. 
As the use of CM in education is a large, diverse, and complex topic, this project focuses on CM use in biology education. 
The results are used to draw conclusions on the efficacy of CM in biology education and to provide guidance for future research. 

<!-- maybe exclude this 
CM proponents in science education consistently evoke constructivist epistemology and Ausubelian theoretical constructs (e.g., meaningful learning) to ground empirical claims about the effectiveness of CM 
[@AMS+ConceptMapEvolutionary2019a; @KinConceptMappingBiology2000; @NGLearningHowLearn1984].
-->

# Concept Mapping in Context

Proponents of various “concept mapping”, “mind mapping”, and “knowledge mapping” activities have promoted their innovations – each with some success – to the education profession and the general public. 
As a result, a confusion of terms has set in which creates difficulties for research on CM [@AhlVarietiesConceptMapping2004; @KinConceptMappingLearning2014]. 
The brief description of CM given in the introduction – as a diagram of conceptual nodes with connecting verbal links – serves as the definition of CM for this project. In contrast, mind mapping is a simple association method without a definitive structure or the use of verbal links [@AhlVarietiesConceptMapping2004]. 
Knowledge mapping is very similar to CM except that it uses a definite set of terms for links between nodes [@ODHKnowledgeMapsScaffolds2001]. 

CM was developed by researchers as a means of representing children’s science knowledge [@NCtoc2007] and has been used by researchers in other contexts and applications as an instrument or method. 
For example, CM has been used to measure conceptual change [@WMConceptMapResearch1990] and as a method of planning program evaluations [@TroIntroductionConceptMapping1989] and dissertation projects [@DonSystematicReviewConcept2017].
Early proponents of CM soon applied CM to instructional purposes as a means of promoting Ausubel’s meaningful learning construct via the process of assimilation – integration of new concepts into existing conceptual frameworks [@NCtoc2007].

The instructional uses of CM are varied. 
From the earliest years, CM was recommended as a learning and formative assessment tool [@NovApplyingLearningPsychology1981]. 
Others applied CM as an advance organizer – an overview of and bridge between the student’s prior knowledge and content to be learned [@WMConceptMapAdvance1991]. 
CM has been used in both a collaborative group setting and with individual learners [@GSLTReviewStudiesCollaborative2007], with students constructing their own maps or studying teacher-constructed maps [@DHM+ExamplebasedLearningComparing2015]. 
Lastly, CM has been applied extensively in higher education [@KinConceptMappingLearning2014], but also in K-12 schooling [@SNAAStudyingConstructingConcept2018a].

While CM has been used as an instructive tool in many school subjects, its first use was in biology education. 
The first article promoting the instructional use of CM was published in The American Biology Teacher [@SVRcmt1979].
Some researchers believe that CM can help students summarize course content using the large vocabulary required in introductory biology courses, thus promoting meaningful rather than rote learning [@JMLinkingPhrasesConcept2019]. 
As stated by @STConceptMappingInstructional1990a, “Concept mapping … appears to be ideally suited to address biology content.” (78-79). 

# Theoretical Framework for the Present Study

Novak worked closely with David Ausubel and later with Bob Gowin (an American philosopher of education at Cornell) coauthoring a book with each [@ANHEducationalPsychologyCognitive1978; @NGLearningHowLearn1984]. 
Ausubel proposed the theory of meaningful learning which would later be closely associated with schema theory and cognitive information processing [@DriPsychologyLearningInstruction2005]. 
Gowin invented the Vee heuristic as a means of making knowledge construction explicit, starting with objects/events and applying concepts, theories, and practices to build up to knowledge claims [@NGLearningHowLearn1984].

Concept mapping has either been framed as an implementation of either cognitive information processing models or constructivist theories of learning. 
Novak adopted his own subtype of constructivism, “human constructivism”, which was his attempt at unifying phenomena in psychology and epistemology. 
In his first proposal of human constructivism, Novak includes CM as a means of representing cognitive structures and of demonstrating the constructivist process of learning [@NovHumanConstructivismUnification1993]. 
In contrast, the work of Karpicke and colleagues has considered CM as an elaborative encoding tool which may be effective to the extent that memory is retrieved, processed in the mapping task, and re-encoded [@BKLearningRetrievalbasedConcept2014; @KBRetrievalPracticeProduces2011a]. 

The competing explanatory frameworks of Karpicke and Novak demonstrates that there is room for differing interpretations of the efficacy of CM and of relevant moderating factors. 
The present study adopts the perspective of constructivism as this is the most prevalent theoretical framework in CM intervention research. 
Novak and his colleagues consider human constructivism to be a moderate epistemological position between logical-positivism and social or radical constructivism [@MWNTeachingScienceUnderstanding1998]. They summarized it in three statements:

  - Human beings are meaning makers.
  - The goal of education is to construct shared meanings.
  - The construction of shared meanings may be facilitated by well-prepared teachers.

# Previous Meta-Analyses

A series of systematic reviews have examined the CM literature each with a focus on a particular aspect or application of CM [e.g., @HSBSystematicReviewConcept2018a, @MCConceptMappingBenefits2020, and @MMSCriticalReviewConcept2015). 
A number of meta-analyses have also been conducted on the effects of CM interventions. 
Most recently, @BFTSDevelopmentStudentsCritical2022 synthesized the metacognitive (i.e., critical thinking) and affective effects of CM interventions across 21 studies. 
@YZZJEffectivenessConceptMapping2017 similarly synthesized metacognitive effects but limited their scope to studies in medical education. 
@ErdInvestigationEffectivenessConcept2016 synthesized cognitive outcomes (i.e., learning gains or academic achievement) but limited their scope to include only studies in Turkey. 

@NALearningConceptKnowledge2006a have conducted the largest and most comprehensive meta-analysis of cognitive and affective outcomes which was updated in 2018 to include over 142 independent effect sizes (ES) from 118 studies [@SNAAStudyingConstructingConcept2018a]. 
They included all experimental and quasi-experimental studies that contrasted CM with other learning activities regardless of pedagogical setting or academic discipline. 
Their analysis yielded an overall mean ES of 0.58 with an I2 of 87.5%, after adjusting the value of two ES identified as outliers. 
They conducted moderator analyses with five categorical variables using the full set of independent effect sizes. 
They then divided the dataset by CM type (constructed or studied) and conducted moderator analyses by the four remaining variables plus three additional categorical variables. 

The largest differences in CM effects were seen for different comparison conditions, region, and level of student interaction.  
No other statistically significant and potentially meaningful differences were seen except for duration of the intervention. The results of the moderator analyses are shown in Table 1.

<!-- Table 1 -->

``` {r}
dat <- read_csv(here::here("writing", "table1.3.csv"))

sig_cols <- 1
sig_rows <- 1

# sig_cols <- c(4,6,6,4,6,4,6)
# sig_rows <- c(1,1,2,3,3,8,8)

tab_gt <- dat |> 
  gt(
    rowname_col = "mod_variable",
    groupname_col = "mod_category"
    # row_group_as_column = TRUE
  ) |> 
  tab_header(
    title = "Table 1",
    subtitle = "Moderator Analysis Results by CM Activity Type"
  ) |> 
  tab_stubhead(label = "Moderator") |> 
  # # This doesn't work at the moment for pdf output and is a reported bug with gt
  # tab_stub_indent(
  #   rows = everything(),
  #   indent = 2
  # ) |> 
  tab_spanner(
    label = "Constructed",
    columns = 3:4
  ) |> 
  tab_spanner(
    label = "Studied",
    columns = 5:6
  ) |> 
  cols_label(
    k_constructed = "k",
    g_constructed = "g",
    k_studied = "k",
    g_studied = "g"
  ) |> 
  tab_source_note(source_note = "Results from Schroeder et al. (2018)") |> my_apa_style()
  # tab_footnote(
  #   footnote = "p < 0.05",
  #   locations = cells_body(columns = sig_cols, rows = sig_rows)
  # ) |> 

tab_gt
```

<!-- Was worth exploring, and may still work, but is finicky
```{r}
# A means of using `flextable` rather than `gt` to create tables
# Unfortunately, it is meant for producing tables from raw data and is not suited to displaying bespoke tables produced as a spreadsheet

library(flextable)

dat <- read_csv(here::here("writing", "table1.3.csv"))

tab_flex <- as_grouped_data(dat, groups = "mod_category") |> 
  as_flextable()

tab_flex
```
-->

# Potential Moderators of CM Efficacy

In this section, the variables investigated in the present study are described and related to Novak’s human constructivism and other considerations from the research synthesis literature. 
Variables were sorted into four groups according to their significance to CM theory and practice. 
This scheme was derived from the ideas of @LipIdentifyingPotentiallyInteresting2019 and the MUTOS framework of @BecImprovingDesignUse2017.

## Extrinsic Characteristics 

Extrinsic characteristics are those characteristics of included studies which concern the researcher or research process.
Differences in mean effect sizes by publication status (i.e. journal article or dissertation/thesis) are known and described as a type of publication bias [@CHVhrs2019].
In an empirical investigation of youth psychotherapy trials, @MWude2004 found that studies reported in dissertations have a lower mean effect size than studies reported in journal articles. 
Variation in publication year may be related to characteristics of the methodology or intervention as practices have evolved. 
It is likely that such variation exists given the age of CM research as a research topic and instructional practices and so publication year may moderate effects. 
<!-- I am not sure that I am buying this anymore -->
The country or region where the study was conducted is included as an extrinsic characteristic. 
While the country or region where a study was conducted may serve as an indicator of socio-cultural factors, it was judged that it is more likely associated with extrinsic or methodological factors than intervention or sample factors.
Thus, region was included as an extrinsic factor.

## Methodological Characteristics

In randomized controlled trials (RCTs) and quasi-experimental designs (QEDs) a control or comparison condition is used to remove the influence of factors unrelated to the treatment. 
@SNAAStudyingConstructingConcept2018a coded a variety of comparison conditions and concluded that larger CM effects are associated with lecture and discussion comparisons as opposed to various review activities such as creating or studying lists or texts. 
It is possible that CM is both more effective than “business-as-usual” (BAU) methods – such as lecturing – and less effective than other evidence-based methods – such as retrieval practice [see @KBRetrievalPracticeProduces2011a].

A particular concern that some scholars have with studies using experimental designs is that important contextual factors will be controlled or ignored, thus leaving the researcher ignorant of their importance to the CM intervention [@KinConceptMappingLearning2014]. 
Some studies take place in a controlled laboratory setting [e.g., @KBRetrievalPracticeProduces2011a] while others assign whole course sections to each condition. In line with general constructivist thought, CM proponents expect that findings from laboratory settings will not generalize to classroom instruction, perhaps underestimating the efficacy of CM (Mintzes et al., 2011; J. D. Novak & Gowin, 1984).

## Intervention Characteristics

CM interventions may require students to construct their own maps or study map(s) constructed by the interventionist. Also, CM interventions may involve interactive or animated maps facilitated by a digital CM tool such as CmapTools (Cañas et al., 2004) or static maps created and presented digitally or on a writing surface. While student construction of concept maps is associated with larger effects, the mode of implementation (interactive, animated, or static) is not associated with different effects (Schroeder et al., 2018).

CM interventions where students construct maps may involve an initial training period where students become familiar with the process as recommended by CM proponents (Kinchin, 2011; Mintzes et al., 2011; J. D. Novak & Gowin, 1984). However, the extent of training may vary from minutes to weeks (Karpicke & Blunt, 2011b). It is not known what effect, if any, that training has in CM efficacy.

The duration of CM interventions vary considerably from less than one week to more than four weeks. In line with the importance of CM training, some believe that CM interventions of a longer duration will have more pronounced or long-lasting effects (Mintzes et al., 2011).  Schroeder et al. (2018) concluded that longer intervention durations are associated with larger effects.

Different strands of constructivism differ on the importance of student social interaction in learning. It appears uncommon to directly examine a CM condition with and without cooperative student interaction as Okebuloa (1992) did or to state a clear rationale for designing an intervention. Schroeder et al. (2018) examined the level of collaboration between learners as a moderator of CM efficacy and found substantial differences between coded levels, though this was not statistically significant.  

## Sample Characteristics

As mentioned above, CM has been applied at different grade levels, across school subjects, and in varying contexts. A large majority of CM research appears to have been conducted in STEM subjects rather than non-STEM subjects, though there is no statistically significant or meaningful difference between the two. Likewise, CM research has been conducted across different grade levels. Per the conclusions of Schroeder et al. (2018), there doesn’t seem to be a substantial or statistically significant difference from intermediate to postsecondary grade levels.

# Rationale and Research Questions

The work of Schroeder et al. (2018) is excellent for its contextualization of CM within educational theories and in its discussion of theory vs. evaluation-orientated research. However, it falls short in its meta-analytic methods in important points. First, in calculating effect sizes (ES) for studies with multiple CM conditions, they calculated synthetic ESs which averaged the means and standard deviations of each CM condition. In cases where outcome measures are reported at multiple time points, Schroeder et al. deferred to the last measurement to calculate the ES. Both ES extraction decisions avoid combining dependent ESs in the meta-analysis but at the cost of lower statistical power, a lesser ability to explain heterogeneous effects, and the potential for bias (COOPER, 2019). They can be avoided using multilevel and/or multivariate meta-analyses (Tipton et al., 2019; Van den Noortgate et al., 2015).

Second, Schroeder et al. do not distinguish between cognitive and affective (referred to as motivational) outcomes in their analysis. Instead, they include both outcomes in a univariate meta-analytic model which makes it difficult if not impossible to make meaningful interpretations of their results. It is conceivable, if not probable, that different socio-cultural and cognitive factors are important for each outcome type. Also, the measures for each outcome type can be expected to differ in important ways (e.g., recall or problem-solving behavior vs. self-report likert items). If these outcome types are to be included in the same meta-analysis, each outcome could be analyzed simultaneously using a multivariate model.

Third, Schroeder et al. identified outliers using a formal method, but one based on unweighted ES which does not take variances or the effect of moderators into account. Smaller studies with larger variances may, by chance, have large ES estimates and have little effect on estimation of the meta-analytic model. Schroeder et al. also did not examine these identified outliers or conduct sensitivity analyses to determine the robustness of their findings. Viechtbauer and Cheung (2010) recommend the use of leave-1-out diagnostic statistics (such as studentized deleted residuals or an analog of Cook’s distances) to identify outliers or influential cases and the use of sensitivity analyses in all meta-analytic procedures.

Fourth, Schroeder et al. used subgroup analysis to investigate all moderators of CM effectiveness using only one variable (CM type) in multi-way analyses, reexamining all other moderators. This causes three problems: 1) inflated family-wise type I error, 2) no estimation of residual or unexplained heterogeneity, and 3) an inability to conduct statistical comparisons between moderators. It has long been known that meta-regression using categorical covariates is equivalent to subgroup analysis but also allows for residual heterogeneity to be estimated and for statistical comparisons to be made (Thompson & Higgins, 2002). As recommended by Tipton, Pustejovsky, and Ahmadi (2019), meta-regression models can include many moderator variables controlling family-wise type I error and partialling out all moderator effects.

In this meta-analysis, these limitations were addressed using current best practice in meta-analysis. The use of meta-regression with multilevel-mixed-effects models containing relevant moderators allowed the following research questions to be investigated:

::: {.content-visible unless-format="apaquarto-docx"}
\begin{enumerate}[{RQ}1.]
\item What is the mean effect size for CM interventions in biology education and to what extent does it vary within and between studies?
\item After controlling for study-level, extrinsic and methodological characteristics, to what extent does the mean effect size vary within and between studies?
\item What is the relationship between effect size and characteristics of the intervention and sample for CM interventions in biology education controlling for study-level, extrinsic and methodological characteristics?
\end{enumerate}
:::

::: {.content-visible when-format="apaquarto-docx"}
  1. What is the mean effect size for CM interventions in biology education and to what extent does it vary within and between studies?
  
  2. After controlling for study-level, extrinsic and methodological characteristics, to what extent does the mean effect size vary within and between studies?
  
  3. What is the relationship between effect size and characteristics of the intervention and sample for CM interventions in biology education controlling for study-level, extrinsic and methodological characteristics?
:::

# Methods

As with all systematic reviews, this study started with a systematic literature search and screen for relevant records of research as defined by criteria that addressed the research questions. These records and the studies they reported were coded according to a prespecified scheme and statistical information for ES calculation were extracted. The results were then analyzed in multi-level, meta-regression models which took into account the hierarchical structure of the data with multiple effects often reported for each study. Influential cases were identified and sensitivity analyses were conducted to check the robustness of the results to them and to assumptions made during ES calculation. All analyses were run in R () principally using the ‘metafor’ () and ‘clubSandwich` packages. All analysis code and meta-analysis data are available at ---------.



```{r}
PRISMA2020_dat <- PRISMA2020::PRISMA_data(read_csv(here::here("data", "lit_search_results.csv")))

PRISMA_diagram <- PRISMA2020::PRISMA_flowdiagram(
  PRISMA2020_dat,
  interactive = FALSE,
  previous = FALSE, # were previous studies sought?
  other = TRUE, # were other studies sought?
  detail_databases = TRUE, # list specific databases?
  detail_registers = FALSE, # list specific registers?
  fontsize = 15,
  font = "Helvetica",
  title_colour = "Goldenrod1",
  greybox_colour = "Gainsboro",
  main_colour = "Black",
  arrow_colour = "Black",
  arrow_head = "normal",
  arrow_tail = "none",
  side_boxes = TRUE # inlcude box labels on side, e.g., "screen", "coding")
)

PRISMA_diagram
# PRISMA_save(PRISMA_diagram, filename ="diagram_MASAL.png", overwrite = TRUE)
```


# References
